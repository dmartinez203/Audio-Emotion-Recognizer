<VSCode.Cell language="markdown">
# Speech Emotion Recognition — Project Notebook

**Selected dataset:** RAVDESS (Ryerson Audio-Visual Database of Emotional Speech and Song)

**Objective:** Build and evaluate models that classify emotion from short speech audio clips. We'll implement at least three different deep-learning methods: 1) CNN for spectrograms, 2) CNN+LSTM hybrid (spatial + temporal), and 3) Transfer-learning baseline using YAMNet embeddings. The implementation uses Python, TensorFlow/Keras, and Librosa.

**Environment note:** You indicated GPU access — the notebook will assume a GPU-enabled TensorFlow installation. If you need instructions for setting up GPU-enabled TensorFlow on macOS, I can add them.
</VSCode.Cell>

<VSCode.Cell language="markdown">
## Notebook Outline

1. Problem statement and quantitative formulation
2. Dataset description (RAVDESS) and download instructions
3. Exploratory data analysis and descriptive statistics
4. Preprocessing & feature extraction (Mel-spectrograms, MFCCs)
5. Models: CNN baseline, CNN+LSTM hybrid, Transfer-learning (YAMNet)
6. Training, evaluation metrics, and comparison
7. Hyperparameter tuning plan
8. Results, discussion, limitations, and references
</VSCode.Cell>

<VSCode.Cell language="python">
# Cell 1: Install dependencies (run once)
# Uncomment and run if packages are missing. Keep commented for faster iteration if already installed.
# !pip install --upgrade pip
# !pip install tensorflow==2.13.1 librosa==0.10.0a4 matplotlib seaborn scikit-learn pandas soundfile tensorflow-hub

# Optional (for YAMNet in TF Hub):
# !pip install tensorflow_io

print("Dependencies: tensorflow, librosa, matplotlib, seaborn, scikit-learn, pandas, soundfile, tensorflow-hub")
</VSCode.Cell>

<VSCode.Cell language="python">
# Cell 2: Imports
import os
import glob
import random
import math
from pathlib import Path
import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
import seaborn as sns
import librosa
import librosa.display
import soundfile as sf
import tensorflow as tf
import tensorflow_hub as hub
from sklearn.model_selection import train_test_split
from sklearn.metrics import classification_report, confusion_matrix, f1_score, accuracy_score

print("TensorFlow version:", tf.__version__)
</VSCode.Cell>

<VSCode.Cell language="markdown">
## Dataset: RAVDESS

- RAVDESS is a labeled emotional speech + song dataset. We'll use the speech subset (audio files in WAV format).
- Emotions in RAVDESS (speech) include: neutral, calm, happy, sad, angry, fearful, disgust, surprised.

Download instructions (manual or scripted):
- Manual: download the RAVDESS zip from Zenodo and extract into a local `data/` folder.
- Scripted (example):

`wget https://zenodo.org/record/1188976/files/Audio_Speech_Actors_01-24.zip`

After extracting, point `DATA_DIR` to the folder containing the WAV files.
</VSCode.Cell>

<VSCode.Cell language="python">
# Cell 3: Paths and configuration
BASE_DIR = Path.cwd()
DATA_DIR = BASE_DIR / "ravdess_data"    # <-- set this after you download & extract RAVDESS
SAMPLE_RATE = 16000
DURATION = 3.0   # seconds: we'll pad/trim audio to a fixed length
SAMPLES = int(SAMPLE_RATE * DURATION)

print("Base dir:", BASE_DIR)
print("Set DATA_DIR to your RAVDESS folder before running heavy cells.")
</VSCode.Cell>

<VSCode.Cell language="python">
# Cell 4: Helper to parse RAVDESS filenames and load metadata
# RAVDESS filename format (example): '03-01-05-01-02-01-12.wav'
# Fields: Modality(01) / Vocal channel / Emotion / ... (see RAVDESS docs)

def parse_ravdess_filename(filename):
    # filename should be basename without path
    parts = filename.replace('.wav','').split('-')
    # emotion index is the 3rd field (index 2) according to naming scheme
    emotion_idx = int(parts[2])
    # mapping from RAVDESS emotion index to label
    mapping = { '01': 'neutral', '02': 'calm', '03': 'happy', '04': 'sad', '05': 'angry', '06': 'fearful', '07': 'disgust', '08': 'surprised' }
    # but parts[2] may be like '05' or '03'
    emotion_code = parts[2]
    label = mapping.get(emotion_code.zfill(2), 'unknown')
    return label

# Quick scan to build metadata table (only if DATA_DIR exists)

def build_metadata_table(data_dir):
    files = list(Path(data_dir).rglob('*.wav'))
    rows = []
    for f in files:
        label = parse_ravdess_filename(f.name)
        rows.append({'path':str(f), 'label':label, 'filename':f.name})
    df = pd.DataFrame(rows)
    return df

# Example (do not run until you set DATA_DIR and downloaded the dataset):
# df = build_metadata_table(DATA_DIR)
# df.head()
</VSCode.Cell>

<VSCode.Cell language="markdown">
## Preprocessing & Feature Extraction

We'll implement functions to:
- Load WAV files, resample to 16 kHz, pad/trim to fixed duration
- Compute Mel-spectrograms and MFCCs using Librosa
- Normalize features (per-sample or dataset-level)
</VSCode.Cell>

<VSCode.Cell language="python">
# Cell 5: Audio utilities and feature extraction

def load_audio(file_path, sr=SAMPLE_RATE, duration=DURATION):
    wav, _ = librosa.load(file_path, sr=sr, mono=True)
    # pad or trim
    if len(wav) < SAMPLES:
        pad_len = SAMPLES - len(wav)
        wav = np.pad(wav, (0, pad_len), mode='constant')
    else:
        wav = wav[:SAMPLES]
    return wav


def compute_mel_spectrogram(wav, sr=SAMPLE_RATE, n_mels=128, n_fft=1024, hop_length=256):
    S = librosa.feature.melspectrogram(y=wav, sr=sr, n_mels=n_mels, n_fft=n_fft, hop_length=hop_length)
    S_db = librosa.power_to_db(S, ref=np.max)
    return S_db


def compute_mfcc(wav, sr=SAMPLE_RATE, n_mfcc=40, n_fft=1024, hop_length=256):
    mfcc = librosa.feature.mfcc(y=wav, sr=sr, n_mfcc=n_mfcc, n_fft=n_fft, hop_length=hop_length)
    return mfcc

# Example usage (after dataset available):
# wav = load_audio(df.loc[0,'path'])
# mel = compute_mel_spectrogram(wav)
# print(mel.shape)
</VSCode.Cell>

<VSCode.Cell language="python">
# Cell 6: Descriptive analysis helpers (class distribution and sample spectrogram)

def plot_class_distribution(df, label_col='label'):
    plt.figure(figsize=(8,4))
    sns.countplot(x=label_col, data=df, order=sorted(df[label_col].unique()))
    plt.xticks(rotation=45)
    plt.title('Class distribution')
    plt.show()


def plot_example_spectrogram(path):
    wav = load_audio(path)
    mel = compute_mel_spectrogram(wav)
    plt.figure(figsize=(8,4))
    librosa.display.specshow(mel, sr=SAMPLE_RATE, hop_length=256, x_axis='time', y_axis='mel')
    plt.colorbar(format='%+2.0f dB')
    plt.title('Mel-spectrogram')
    plt.show()

# After building metadata df: plot_class_distribution(df)
# and plot_example_spectrogram(df.iloc[0].path)
</VSCode.Cell>

<VSCode.Cell language="markdown">
## Model 1: CNN (spectrogram classification) & Model 2: CNN + LSTM (hybrid)

We'll create configurable Keras model builders so we can compare:
- Pure CNN operating on 2D mel-spectrogram inputs
- CNN for feature extraction + reshape/time-distributed + LSTM to model temporal structure

Both models will be trained using categorical crossentropy for multi-class classification.
</VSCode.Cell>

<VSCode.Cell language="python">
# Cell 7: Keras model builders
from tensorflow.keras import layers, models, regularizers


def build_cnn_model(input_shape, n_classes, dropout=0.3):
    inp = layers.Input(shape=input_shape)
    x = layers.Reshape((*input_shape,1))(inp) if len(input_shape)==2 else inp
    x = layers.Conv2D(32, (3,3), activation='relu', padding='same')(x)
    x = layers.BatchNormalization()(x)
    x = layers.MaxPool2D((2,2))(x)
    x = layers.Conv2D(64, (3,3), activation='relu', padding='same')(x)
    x = layers.BatchNormalization()(x)
    x = layers.MaxPool2D((2,2))(x)
    x = layers.Conv2D(128, (3,3), activation='relu', padding='same')(x)
    x = layers.BatchNormalization()(x)
    x = layers.GlobalAveragePooling2D()(x)
    x = layers.Dropout(dropout)(x)
    out = layers.Dense(n_classes, activation='softmax')(x)
    model = models.Model(inp, out)
    return model


def build_cnn_lstm_model(input_shape, n_classes, dropout=0.3):
    # input_shape expected as (time_frames, n_mels)
    inp = layers.Input(shape=input_shape)
    x = layers.Reshape((input_shape[0], input_shape[1], 1))(inp)
    # apply small CNN over frequency axis per time frame via TimeDistributed
    x = layers.TimeDistributed(layers.Conv1D(64, kernel_size=3, activation='relu'))(inp)
    x = layers.TimeDistributed(layers.MaxPool1D(2))(x)
    x = layers.TimeDistributed(layers.Flatten())(x)
    # now sequence of frame-level embeddings -> LSTM
    x = layers.LSTM(128, return_sequences=False)(x)
    x = layers.Dropout(dropout)(x)
    out = layers.Dense(n_classes, activation='softmax')(x)
    model = models.Model(inp, out)
    return model

# Example shapes:
# If mel spectrogram shape is (n_mels, time_frames) we may transpose to (time_frames, n_mels)
</VSCode.Cell>

<VSCode.Cell language="markdown">
## Transfer-learning baseline: YAMNet embeddings

YAMNet is a pretrained audio event classifier trained on AudioSet. We'll use it to extract embeddings for each waveform and train a small classifier on top. This acts as a strong transfer-learning baseline.
</VSCode.Cell>

<VSCode.Cell language="python">
# Cell 8: YAMNet embedding extraction (tensorflow_hub)

YAMNET_HANDLE = "https://tfhub.dev/google/yamnet/1"

# Load yamnet (this will download its weights the first time)
try:
    yamnet_model = hub.load(YAMNET_HANDLE)
    print('YAMNet loaded successfully')
except Exception as e:
    print('YAMNet load failed — ensure internet and tensorflow-hub installed:', e)

# yamnet_model expects waveform samples at 16kHz and returns scores, embeddings, and log-mel spectrogram
# Usage template (after load):
# scores, embeddings, spectrogram = yamnet_model(waveform)

# We'll create a helper to compute a single embedding per clip by averaging frame-level embeddings

def yamnet_embedding_from_wave(waveform):
    # waveform: 1-D float32 numpy array at 16k
    # Convert to tensor
    waveform_tf = tf.convert_to_tensor(waveform, dtype=tf.float32)
    # Add batch dim if required
    if len(waveform_tf.shape) == 1:
        waveform_tf = tf.reshape(waveform_tf, [1, -1])
    result = yamnet_model(waveform_tf)
    # result[1] is embeddings shape (batch, frames, 1024)
    embeddings = result[1].numpy()
    # average over frames
    emb_avg = np.mean(embeddings, axis=1)
    return emb_avg[0]

# Note: YAMNet returns 1024-dim embeddings per frame; we average to get a fixed-size vector for each clip.
</VSCode.Cell>

<VSCode.Cell language="markdown">
## Training utilities and evaluation
</VSCode.Cell>

<VSCode.Cell language="python">
# Cell 9: Training and evaluation helpers
from tensorflow.keras.callbacks import EarlyStopping, ModelCheckpoint, ReduceLROnPlateau


def compile_and_train(model, train_ds, val_ds, epochs=30, lr=1e-3, model_name='model'):
    model.compile(optimizer=tf.keras.optimizers.Adam(learning_rate=lr),
                  loss='sparse_categorical_crossentropy',
                  metrics=['accuracy'])
    callbacks = [
        EarlyStopping(monitor='val_loss', patience=6, restore_best_weights=True),
        ReduceLROnPlateau(monitor='val_loss', factor=0.5, patience=3),
        ModelCheckpoint(f'{model_name}.h5', save_best_only=True)
    ]
    history = model.fit(train_ds, validation_data=val_ds, epochs=epochs, callbacks=callbacks)
    return history


def evaluate_model(model, X_test, y_test, label_names=None):
    preds = model.predict(X_test)
    y_pred = np.argmax(preds, axis=1)
    acc = accuracy_score(y_test, y_pred)
    f1 = f1_score(y_test, y_pred, average='weighted')
    print('Accuracy:', acc)
    print('Weighted F1:', f1)
    print('\nClassification Report:\n')
    print(classification_report(y_test, y_pred, target_names=label_names))
    cm = confusion_matrix(y_test, y_pred)
    plt.figure(figsize=(8,6))
    sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', xticklabels=label_names, yticklabels=label_names)
    plt.xlabel('Pred')
    plt.ylabel('True')
    plt.title('Confusion Matrix')
    plt.show()

# Note: For large datasets use tf.data pipelines rather than in-memory arrays.
</VSCode.Cell>

<VSCode.Cell language="markdown">
## Hyperparameter tuning plan

- Tune learning rate: [1e-4, 3e-4, 1e-3]
- Batch size: [16, 32, 64]
- CNN depth: [2,3] conv blocks
- LSTM units: [64, 128]

Use a small subset for fast experiments, then scale best config to full train set.
</VSCode.Cell>

<VSCode.Cell language="markdown">
## Diagram (Architecture & Workflow)

Simple ASCII diagram (replace with a plotted block diagram in the notebook if desired):

Audio WAV (16kHz, 3s) --> Preprocess (trim/pad) --> Feature extraction
  - Mel-spectrogram (128 x T) --> CNN --> Dense --> Softmax (CNN baseline)
  - Mel-spectrogram (transpose to T x 128) --> TimeDistributed CNN --> LSTM --> Dense --> Softmax (CNN+LSTM)
  - Raw waveform --> YAMNet (pretrained) embeddings --> Dense classifier (Transfer baseline)
</VSCode.Cell>

<VSCode.Cell language="markdown">
## Next steps (what I'll do next)

- Implement data loader that builds tf.data.Dataset pipelines (mel spectrograms & YAMNet embeddings).
- Run a small experiment (train for a few epochs) to sanity-check code on a subset.
- Produce full EDA plots and begin training runs for the three methods.

If you want, I can now:
- (A) Generate the tf.data pipeline & runnable training cells next, or
- (B) Add a ready-made environment `requirements.txt` + `README.md` and a minimal runnable script.

Which do you prefer I do first?" 
</VSCode.Cell>

<VSCode.Cell language="python">
# Cell 10: Data pipeline builder (tf.data) — builds tf.data.Dataset of (mel, label)
import tensorflow as _tf

# Fixed parameters for spectrogram shapes
N_MELS = 128
HOP_LENGTH = 256
N_FFT = 1024
TIME_FRAMES = int(np.ceil(SAMPLES / HOP_LENGTH))  # expected time frames per clip

def paths_labels_from_df(df, label_col='label'):
    paths = df['path'].tolist()
    labels = df[label_col].tolist()
    # encode labels to ints
    unique = sorted(list(set(labels)))
    label_to_idx = {l:i for i,l in enumerate(unique)}
    y = [label_to_idx[l] for l in labels]
    return paths, y, label_to_idx

def tf_load_mel(path):
    # path is a tf string tensor
    def _load(path_str):
        wav = load_audio(path_str.decode('utf-8'))
        mel = compute_mel_spectrogram(wav, sr=SAMPLE_RATE, n_mels=N_MELS, n_fft=N_FFT, hop_length=HOP_LENGTH)
        # normalize per-sample (min-max to [0,1])
        mel = (mel - np.min(mel)) / (np.max(mel) - np.min(mel) + 1e-9)
        return mel.astype(np.float32)

    mel = _tf.py_function(func=_load, inp=[path], Tout=_tf.float32)
    mel.set_shape([N_MELS, TIME_FRAMES])
    return mel

def make_dataset(paths, labels, batch_size=32, shuffle=True, augment=False):
    ds = _tf.data.Dataset.from_tensor_slices((paths, labels))
    if shuffle:
        ds = ds.shuffle(buffer_size=len(paths), reshuffle_each_iteration=True)

    def _map(path, label):
        mel = tf_load_mel(path)
        return mel, label

    ds = ds.map(_map, num_parallel_calls=_tf.data.AUTOTUNE)
    ds = ds.batch(batch_size).prefetch(_tf.data.AUTOTUNE)
    return ds

print('Data pipeline builder ready. TIME_FRAMES=', TIME_FRAMES)
</VSCode.Cell>

<VSCode.Cell language="python">
# Cell 11: Build dataset objects (run after setting DATA_DIR and downloading RAVDESS)
if DATA_DIR.exists():
    df = build_metadata_table(DATA_DIR)
    print('Found', len(df), 'audio files')
    plot_class_distribution(df)
    paths, labels, label_map = paths_labels_from_df(df)
    n_classes = len(label_map)
    print('Label map:', label_map)

    # split
    p_train = 0.7
    p_val = 0.15
    p_test = 0.15
    X_temp, X_test, y_temp, y_test = train_test_split(paths, labels, test_size=p_test, stratify=labels, random_state=42)
    val_size = p_val / (p_train + p_val)
    X_train, X_val, y_train, y_val = train_test_split(X_temp, y_temp, test_size=val_size, stratify=y_temp, random_state=42)

    print('Train/Val/Test sizes:', len(X_train), len(X_val), len(X_test))

    BATCH_SIZE = 32
    train_ds = make_dataset(X_train, y_train, batch_size=BATCH_SIZE, shuffle=True)
    val_ds = make_dataset(X_val, y_val, batch_size=BATCH_SIZE, shuffle=False)
    test_ds = make_dataset(X_test, y_test, batch_size=BATCH_SIZE, shuffle=False)

    # Inspect a single batch
    for xb, yb in train_ds.take(1):
        print('Batch mel shape:', xb.shape)
        print('Batch labels shape:', yb.shape)
else:
    print('DATA_DIR not found at', DATA_DIR)
    print('Please download RAVDESS and extract to that path. Example (shell):')
    print('\ncurl -L -o Audio_Speech_Actors_01-24.zip "https://zenodo.org/record/1188976/files/Audio_Speech_Actors_01-24.zip"')
    print('unzip Audio_Speech_Actors_01-24.zip -d ravdess_data')
</VSCode.Cell>

<VSCode.Cell language="python">
# Cell 12: Quick sanity-train demo (train CNN for 2 epochs on small subset)
if DATA_DIR.exists():
    # ensure n_classes defined
    try:
        n_classes
    except NameError:
        _, _, label_map = paths_labels_from_df(build_metadata_table(DATA_DIR))
        n_classes = len(label_map)

    input_shape = (N_MELS, TIME_FRAMES)
    model = build_cnn_model(input_shape, n_classes)
    model.summary()

    # Train on a small number of steps for a quick sanity check
    steps = 50
    val_steps = 10
    history = model.fit(train_ds.take(steps), validation_data=val_ds.take(val_steps), epochs=2)
    print('Sanity training finished')
else:
    print('DATA_DIR not available. Skipping demo training.')
</VSCode.Cell>